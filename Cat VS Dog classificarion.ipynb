{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)\n",
    "\n",
    "#rescale  = convert into minmax scaler\n",
    "#shear raange = if photo till upto 20% it can detect it\n",
    "#zoom range = some pics are sqinged so, it will zoom and recognised\n",
    "# it genaret the data from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_gen.flow_from_directory(r\"D:\\ds exel files\\cnn dataset\\training_set\", target_size = (64,64), \n",
    "                                                                                                            class_mode= 'binary')\n",
    "\n",
    "#target size= it creat 64*64 picsal\n",
    "#class_mode= we have cat and dog thats why binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_indices\n",
    "# it considar cat as 0\n",
    "# it consider dog as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the test deta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_gen.flow_from_directory(r\"D:\\ds exel files\\cnn dataset\\test_set\", target_size = (64,64), class_mode= 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling\n",
    "### Building the convolution neural network\n",
    "- convolution reduced the input nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the CNN\n",
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "classifier.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))\n",
    "\n",
    "# filter=32, ie, nomber of filter used are 32\n",
    "# kernel_size=3, ie, each filter is (3*3) pixel\n",
    "# input_shape = [64,64,3], ie, colour image thats why it 3d, and 3 indicates red, blue, green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2 - pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "#poolsize= 2, ie, poolsize(2*2)\n",
    "#stride=2, ie, 2lineswhich make pool size (2*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size= 2, strides=2))\n",
    "\n",
    "#for getting more accuret result you can incrrese convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### third convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size= 2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatterning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full conection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "classifier.add(Dense(units=128, activation='relu'))# hidden layer with neurons\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))# output layer with 1 neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trining the CNN model with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "252/252 [==============================] - 110s 431ms/step - loss: 0.6737 - accuracy: 0.5757 - val_loss: 0.6444 - val_accuracy: 0.6125\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - 46s 183ms/step - loss: 0.6097 - accuracy: 0.6636 - val_loss: 0.5845 - val_accuracy: 0.7085\n",
      "Epoch 3/25\n",
      "252/252 [==============================] - 60s 240ms/step - loss: 0.5512 - accuracy: 0.7184 - val_loss: 0.5341 - val_accuracy: 0.7350\n",
      "Epoch 4/25\n",
      "252/252 [==============================] - 47s 185ms/step - loss: 0.5123 - accuracy: 0.7460 - val_loss: 0.5154 - val_accuracy: 0.7415\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - 49s 194ms/step - loss: 0.4857 - accuracy: 0.7667 - val_loss: 0.5437 - val_accuracy: 0.7470\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - 47s 188ms/step - loss: 0.4687 - accuracy: 0.7766 - val_loss: 0.4923 - val_accuracy: 0.7640\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - 48s 190ms/step - loss: 0.4381 - accuracy: 0.7949 - val_loss: 0.5037 - val_accuracy: 0.7500\n",
      "Epoch 8/25\n",
      "252/252 [==============================] - 46s 181ms/step - loss: 0.4134 - accuracy: 0.8131 - val_loss: 0.4603 - val_accuracy: 0.7775\n",
      "Epoch 9/25\n",
      "252/252 [==============================] - 46s 184ms/step - loss: 0.3979 - accuracy: 0.8170 - val_loss: 0.4564 - val_accuracy: 0.7910\n",
      "Epoch 10/25\n",
      "252/252 [==============================] - 45s 180ms/step - loss: 0.3771 - accuracy: 0.8259 - val_loss: 0.4703 - val_accuracy: 0.7860\n",
      "Epoch 11/25\n",
      "252/252 [==============================] - 47s 187ms/step - loss: 0.3526 - accuracy: 0.8392 - val_loss: 0.5129 - val_accuracy: 0.7750\n",
      "Epoch 12/25\n",
      "252/252 [==============================] - 47s 185ms/step - loss: 0.3363 - accuracy: 0.8477 - val_loss: 0.5404 - val_accuracy: 0.7625\n",
      "Epoch 13/25\n",
      "252/252 [==============================] - 45s 180ms/step - loss: 0.3179 - accuracy: 0.8571 - val_loss: 0.4967 - val_accuracy: 0.7790\n",
      "Epoch 14/25\n",
      "252/252 [==============================] - 46s 184ms/step - loss: 0.3060 - accuracy: 0.8677 - val_loss: 0.4793 - val_accuracy: 0.8005\n",
      "Epoch 15/25\n",
      "252/252 [==============================] - 45s 178ms/step - loss: 0.2714 - accuracy: 0.8828 - val_loss: 0.4758 - val_accuracy: 0.8020\n",
      "Epoch 16/25\n",
      "252/252 [==============================] - 46s 184ms/step - loss: 0.2571 - accuracy: 0.8938 - val_loss: 0.5403 - val_accuracy: 0.7950\n",
      "Epoch 17/25\n",
      "252/252 [==============================] - 47s 185ms/step - loss: 0.2441 - accuracy: 0.8951 - val_loss: 0.5121 - val_accuracy: 0.7950\n",
      "Epoch 18/25\n",
      "252/252 [==============================] - 46s 182ms/step - loss: 0.2174 - accuracy: 0.9113 - val_loss: 0.5318 - val_accuracy: 0.7865\n",
      "Epoch 19/25\n",
      "252/252 [==============================] - 49s 195ms/step - loss: 0.2019 - accuracy: 0.9197 - val_loss: 0.5694 - val_accuracy: 0.7990\n",
      "Epoch 20/25\n",
      "252/252 [==============================] - 46s 182ms/step - loss: 0.1943 - accuracy: 0.9172 - val_loss: 0.5441 - val_accuracy: 0.7965\n",
      "Epoch 21/25\n",
      "252/252 [==============================] - 46s 181ms/step - loss: 0.1730 - accuracy: 0.9292 - val_loss: 0.6018 - val_accuracy: 0.7960\n",
      "Epoch 22/25\n",
      "252/252 [==============================] - 46s 183ms/step - loss: 0.1644 - accuracy: 0.9330 - val_loss: 0.5790 - val_accuracy: 0.7985\n",
      "Epoch 23/25\n",
      "252/252 [==============================] - 47s 185ms/step - loss: 0.1462 - accuracy: 0.9426 - val_loss: 0.6664 - val_accuracy: 0.7930\n",
      "Epoch 24/25\n",
      "252/252 [==============================] - 46s 182ms/step - loss: 0.1331 - accuracy: 0.9482 - val_loss: 0.6630 - val_accuracy: 0.7985\n",
      "Epoch 25/25\n",
      "252/252 [==============================] - 46s 184ms/step - loss: 0.1259 - accuracy: 0.9518 - val_loss: 0.6634 - val_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7c55c4648>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = train_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open(r\"D:\\data science and AI\\excel files\\cnn dataset\\images.jfif\")\n",
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "#axis =0, ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 160ms/step\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
